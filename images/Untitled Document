<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>Alan Luo</title>

  <!-- Latest compiled and minified CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

  <!-- Custom CSS -->
  <link href="sass/style.css" rel="stylesheet">
<style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 5px 0px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 5px; -webkit-border-radius: 5px; -moz-border-radius: 5px; -khtml-border-radius: 5px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 1px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: .7em}
.MathJax_MenuRadioCheck.RTL {right: .7em; left: auto}
.MathJax_MenuLabel {padding: 1px 2em 3px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #DDDDDD; margin: 4px 3px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: #606872; color: white}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style></head>

<!-- The #page-top ID is part of the scrolling feature - the data-spy and data-target are part of the built-in Bootstrap
scrollspy function -->

<body id="page-top" data-spy="scroll" data-target=".navbar-fixed-top"><div id="MathJax_Message" style="display: none;"></div>

<!-- Navigation -->
<nav class="navbar navbar-default navbar-fixed-top top-nav-collapse" role="navigation">
  <div class="container">
    <div class="navbar-header page-scroll">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand page-scroll" href="#page-top">Alan Luo</a>
    </div>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse navbar-ex1-collapse">
      <ul class="nav navbar-nav">
        <!-- Hidden li included to remove active class from about link when scrolled up past about section -->
        <li class="hidden">
          <a class="page-scroll" href="#page-top"></a>
        </li>
        <li class="active">
          <a class="page-scroll" href="#about">About</a>
        </li>
        <li>
          <a class="page-scroll" href="#publication">Publications</a>
        </li>
      </ul>
    </div>
    <!-- /.navbar-collapse -->
  </div>
  <!-- /.container -->
</nav>

<!-- Section: Introduction -->
<section id="intro" class="section-intro">
  <div class="container-fluid">
    <div class="row">
      <div class="col-xs-6 col-xs-offset-3 my-info">
        <a href="https://youtu.be/dQw4w9WgXcQ"></a><h1><a href="https://youtu.be/dQw4w9WgXcQ">Zelun (Alan) Luo</a></h1>
        <p>
          PhD Student<br>
          <a href="http://svl.stanford.edu/">Stanford Vision and Learning Lab</a><br>
          Advisor: <a href="https://profiles.stanford.edu/fei-fei-li">Prof. Fei-Fei Li</a><br>
          alanzluo at stanford dot edu<br>
          <a href="https://scholar.google.com/citations?user=MVUbYCkAAAAJ&amp;hl=en">[Google Scholar]</a>
          <a href="https://github.com/d1ngn1gefe1">[Github]</a>
          <a href="intro/cv.pdf">[CV (updated on July 2018)]</a>
        </p>
      </div>
    </div>
  </div>
</section>

<!-- Section: About -->
<section id="about" class="section-about">
  <div class="container-fluid">
    <div class="row">
      <h1 class="subtitle">About</h1>
    </div>

    <div class="row">
      <p class="text-left">
        I am a second-year PhD student in the Computer Science department at Stanford University. I am working in the <a href="http://vision.stanford.edu/">Stanford Vision and Learning Lab</a>, advised by <a href="http://vision.stanford.edu/feifeili/">Prof. Fei-Fei Li</a>.
      </p>
      <p class="text-left">
        I am a member of the <a href="https://aicare.stanford.edu/">Stanford Program in AI-Assisted Care (PAC)</a>, which is a collaboration between the Stanford AI Lab and Stanford Clinical Excellence Research Center that aims to use computer vision and machine learning to create AI-assisted smart healthcare spaces.
      </p>
      <p class="text-left">
        Before that, I received my master's degree from Stanford University and my bachelor's degree from the University of Illinois Urbana-Champaign. During my master's study, I worked in the <a href="http://vision.stanford.edu/">Stanford Vision and Learning Lab</a> with <a href="http://vision.stanford.edu/feifeili/">Prof. Fei-Fei Li</a>, and the <a href="http://med.stanford.edu/cerc.html">Clinical Excellence Research Center</a> with <a href="https://med.stanford.edu/profiles/arnold-milstein">Prof. Arnold Milstein</a>. During my undergraduate study, I spent three years reseaching in the <a href="http://light.ece.illinois.edu/">Quantitative Light Imaging Laboratory</a> with <a href="http://light.ece.illinois.edu/">Prof. Gabriel Popescu</a>, and the <a href="http://vision.ai.illinois.edu/">Computer Vision and Robotics Laboratory</a> with <a href="https://filebox.ece.vt.edu/~jbhuang/">Prof. Jia-Bin Huang</a> and <a href="http://vision.ai.illinois.edu/ahuja.html">Prof. Narendra Ahuja</a>.
      </p>
    </div>

    <div class="row education">
      <h3 class="text-left">Education</h3>
      <div class="col-sm-4 col-xs-6 school">
        <img src="about/stanford_seal.png" class="img-fluid school-fig">
        <p>Stanford University</p>
        <p><small>Doctor of Philosophy</small></p>
        <p><small>Computer Science</small></p>
        <p></p>
      </div>
      <div class="col-sm-4 col-xs-6 school">
        <img src="about/stanford_seal.png" class="img-fluid school-fig">
        <p>Stanford University</p>
        <p><small>Master of Science</small></p>
        <p><small>Computer Science</small></p>
        <p></p>
      </div>
      <div class="col-sm-4 col-xs-6 school">
        <img src="about/uiuc_seal.png" class="img-fluid school-fig">
        <p>University of Illinois Urbana-Champaign</p>
        <p><small>Bachelor of Science</small></p>
        <p><small>Electrical and Computer Engineering</small></p>
        <p></p>
      </div>
    </div>

    <div class="row industry">
      <h3 class="text-left">Industry</h3>
      <div class="col-sm-3 col-xs-6 company">
        <img src="about/facebook.png" class="img-fluid company-fig img-circle">
        <p>Facebook Research<br>
        </p><p><small>Research Intern</small></p>
        <p><small>Summer 2019</small></p>
        <p></p>
      </div>
      <div class="col-sm-3 col-xs-6 company">
        <img src="about/google.png" class="img-fluid company-fig img-circle">
        <p>Google Cloud AI<br>
        </p><p><small>Research Intern</small></p>
        <p><small>Summer 2017</small></p>
        <p></p>
      </div>
      <div class="col-sm-3 col-xs-6 company">
        <img src="about/amazon.png" class="img-fluid company-fig img-circle">
        <p>Amazon A9<br>
        </p><p><small>Research Intern</small></p>
        <p><small>Summer 2016</small></p>
        <p></p>
      </div>
      <div class="col-sm-3 col-xs-6 company">
        <img src="about/yahoo.png" class="img-fluid company-fig img-circle">
        <p>Yahoo<br>
        </p><p><small>Software Engineering Intern</small></p>
        <p><small>Summer 2015</small></p>
        <p></p>
      </div>
    </div>

    <div class="row teaching">
      <h3 class="text-left">Teaching</h3>
      <ul>
        <li><p><a href="http://cs337.stanford.edu/">MED 277/CS 337 (AI-Assisted Health Care)</a>: Head Teaching Assistant (Fall 2018); Teaching Assistant (Winter 2018)</p></li>
        <li><p><a href="http://cs231n.stanford.edu/">CS 231N (Convolutional Neural Networks for Visual Recognition)</a>: Teaching Assistant (Spring 2017)</p></li>
        <li><p><a href="http://cs224n.stanford.edu/">CS 224N (Natural Language Processing with Deep Learning)</a>: Teaching Assistant (Winter 2017)</p></li>
        <li><p><a href="http://cs109.stanford.edu/">CS 109 (Probability for Computer Scientists)</a>: Teaching Assistant (Winter 2016 &amp; Spring 2016)</p></li>
        <li><p><a href="http://cs131.stanford.edu/">CS 131 (Computer Vision: Foundations and Applications)</a>: Teaching Assistant (Fall 2015); Head Teaching Assistant (Fall 2016)</p></li>
      </ul>
    </div>

    <div class="row professional">
      <h3 class="text-left">Professional Activities</h3>
      <ul>
      	<li><p>AI Conference Reviewer: CVPR 2020, AAAI 2020, NeurIPS 2019, ICCV 2019, ICML 2019, CVPR 2019, CVPR 2018</p></li>
      	<li><p>Medical Conference Reviewer: MLHC 2019, MLHC 2018</p></li>
        <li><p>Journal Reviewer: TPAMI (Jun. 2019), TPAMI (Nov. 2018)</p></li>
      </ul>
    </div>

  </div>
</section>

<!-- Section: Publication-->
<section id="publication" class="section-publication">
  <div class="container-fluid">
    <div class="row">
      <h1 class="subtitle">Selected Publications</h1>
    </div>

    <div class="row">
      <h3 class="subfield-first">Computer Vision and Deep Learning</h3>

      <div class="row paper">
        <div class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig">
          <img src="publications/luo2018graph.png" class="img-responsive">
        </div>
        <div class="col-sm-9 col-xs-12 paper-info">
          <h4>Graph Distillation for Action Detection with Privileged Information</h4>
          <p><strong>Zelun Luo</strong>, Jun-Ting Hsieh, Lu Jiang, Juan Carlos Niebles, Li Fei-Fei</p>
          <p>European Conference on Computer Vision (ECCV) 2018</p>
          <div>
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="bottom" title="" data-content="In this work, we propose a technique that tackles the video understanding problem under a realistic, demanding condition in which we have limited labeled data and partially observed training modalities. Common methods such as transfer learning do not take advantage of the rich information from extra modalities potentially available in the source domain dataset. On the other hand, previous work on cross-modality learning only focuses on a single domain or task. In this work, we propose a graph-based distillation method that incorporates rich privileged information from a large multi-modal dataset in the source domain, and shows an improved performance in the target domain where data is scarce. Leveraging both a large-scale dataset and its extra modalities, our method learns a better model for temporal action detection and action classification without needing to have access to these modalities during test time. We evaluate our approach on action classification and temporal action detection tasks, and show that our models achieve the state-of-the-art performance on the PKU-MMD and NTU RGB+D datasets." data-original-title="Abstract">Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/1712.00108">PDF</a>
            <a class="btn btn-default btn-xs" target="_blank" href="http://alan.vision/eccv18_graph">Project</a>
            <a class="btn btn-default btn-xs" target="_blank" href="http://alan.vision/publications/luo2018graph_poster.pdf">Poster</a>
            <a class="btn btn-default btn-xs" target="_blank" href="https://github.com/google/graph_distillation">Code</a>
          </div>
        </div>
      </div>

      <div class="row paper">
        <div class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig">
          <img src="publications/zou2018dfnet.gif" class="img-responsive" onerror="this.onerror=null;this.src='publications/zou2018dfnet.gif';">
        </div>
        <div class="col-sm-9 col-xs-12 paper-info">
          <h4>DF-Net: Unsupervised Joint Learning of Depth and Flow using Cross-Network Consistency</h4>
          <p>Yuliang Zou, <strong>Zelun Luo</strong>, Jia-Bin Huang</p>
          <p>European Conference on Computer Vision (ECCV) 2018</p>
          <div>
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="bottom" title="" data-content="We present an unsupervised learning framework for simultaneously training single-view depth prediction and optical flow estimation models using unlabeled video sequences. Existing unsupervised methods often exploit brightness constancy and spatial smoothness priors to train depth or flow models. In this paper, we propose to leverage geometric consistency as additional supervisory signals. Our core idea is that for rigid regions we can use the predicted scene depth and camera motion to synthesize 2D optical flow by backprojecting the induced 3D scene flow. The discrepancy between the rigid flow (from depth prediction and camera motion) and the estimated flow (from optical flow model) allows us to impose a cross-task consistency loss. While all the networks are jointly optimized during training, they can be applied independently at test time. Extensive experiments demonstrate that our depth and flow models compare favorably with state-of-the-art unsupervised methods." data-original-title="Abstract">Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/1809.01649">PDF</a>
            <a class="btn btn-default btn-xs" target="_blank" href="http://yuliang.vision/DF-Net/">Project</a>
            <a class="btn btn-default btn-xs" target="_blank" href="https://github.com/vt-vl-lab/DF-Net">Code</a>
          </div>
        </div>
      </div>

      <div class="row paper">
        <div class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig">
          <img src="publications/luo2017label.png" class="img-responsive">
        </div>
        <div class="col-sm-9 col-xs-12 paper-info">
          <h4>Label Efficient Learning of Transferable Representations across Domains and Tasks</h4>
          <p><strong>Zelun Luo</strong>, Yuliang Zou, Judy Hoffman, Li Fei-Fei</p>
          <p>Conference on Neural Information Processing Systems (NIPS) 2017</p>
          <div>
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="bottom" title="" data-content="We propose a framework that learns a representation transferable across different domains and tasks in a data efficient manner. Our approach battles domain shift with a domain adversarial loss, and generalizes the embedding to novel task using a metric learning-based approach. Our model is simultaneously optimized on labeled source data and unlabeled or sparsely labeled data in the target domain. Our method shows compelling results on novel classes within a new domain even when only a few labeled examples per class are available, outperforming the prevalent fine-tuning approach. In addition, we demonstrate the effectiveness of our framework on the transfer learning task from image object recognition to video action recognition." data-original-title="Abstract">Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/1712.00123">PDF</a>
            <a class="btn btn-default btn-xs" target="_blank" href="http://alan.vision/nips17_label/poster.pdf">Poster</a>
            <a class="btn btn-default btn-xs" target="_blank" href="http://alan.vision/nips17_label/">Project</a>
          </div>
        </div>
      </div>

      <div class="row paper">
        <div class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig">
          <img src="publications/luo2017unsupervised.png" class="img-responsive">
        </div>
        <div class="col-sm-9 col-xs-12 paper-info">
          <h4>Unsupervised Learning of Long-Term Motion Dynamics for Videos</h4>
          <p><strong>Zelun Luo</strong>, Boya Peng, De-An Huang, Alexandre Alahi, Li Fei-Fei</p>
          <p>Conference on Computer Vision and Pattern Recognition (CVPR) 2017</p>
          <div>
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="bottom" title="" data-content="We present an unsupervised representation learning approach
                                              that compactly encodes the motion dependencies
                                              in videos. Given a pair of images from a video clip, our
                                              framework learns to predict the long-term 3D motions. To
                                              reduce the complexity of the learning framework, we propose
                                              to describe the motion as a sequence of atomic 3D
                                              flows computed with RGB-D modality. We use a Recurrent
                                              Neural Network based Encoder-Decoder framework
                                              to predict these sequences of flows. We argue that in order
                                              for the decoder to reconstruct these sequences, the encoder
                                              must learn a robust video representation that captures
                                              long-term motion dependencies and spatial-temporal relations.
                                              We demonstrate the effectiveness of our learned temporal
                                              representations on activity classification across multiple
                                              modalities and datasets such as NTU RGB+D and MSR
                                              Daily Activity 3D. Our framework is generic to any input
                                              modality, i.e., RGB, depth, and RGB-D videos." data-original-title="Abstract">Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/pdf/1701.01821.pdf">PDF</a>
            <a class="btn btn-default btn-xs" target="_blank" href="publications/CVPR2017-poster.png">Poster</a>
          </div>
        </div>
      </div>

      <div class="row paper paper-last">
        <div class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig">
          <img src="publications/haque2016towards.gif" class="img-responsive" onerror="this.onerror=null;this.src='publications/haque2016towards.png';">
        </div>
        <div class="col-sm-9 col-xs-12 paper-info">
          <h4>Towards Viewpoint Invariant 3D Human Pose Estimation</h4>
          <p>Albert Haque, <strong>Zelun Luo*</strong>, Boya Peng*, Alexandre Alahi, Serena Yeung, Li Fei-Fei</p>
          <p>European Conference on Computer Vision (ECCV) 2016</p>
          <div>
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="bottom" title="" data-content="We propose a viewpoint invariant model for 3D human pose
                                              estimation from a single depth image. To achieve this, our discriminative
                                              model embeds local regions into a learned viewpoint invariant feature
                                              space. Formulated as a multi-task learning problem, our model is able to
                                              selectively predict partial poses in the presence of noise and occlusion.
                                              Our approach leverages a convolutional and recurrent network architecture
                                              with a top-down error feedback mechanism to self-correct previous
                                              pose estimates in an end-to-end manner. We evaluate our model on a
                                              previously published depth dataset and a newly collected human pose
                                              dataset containing 100K annotated depth images from extreme viewpoints.
                                              Experiments show that our model achieves competitive performance
                                              on frontal views while achieving state-of-the-art performance on
                                              alternate viewpoints." data-original-title="Abstract">Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/pdf/1603.07076.pdf">PDF</a>
            <a class="btn btn-default btn-xs" target="_blank" href="https://www.alberthaque.com/projects/viewpoint_3d_pose/">Website</a>
          </div>
        </div>
      </div>
    </div>

    <div class="row">
      <h3 class="subfield">AI-Assisted Healthcare</h3>

      <div class="row paper">
        <div class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig">
          <img src="publications/darke2018vision.png" class="img-responsive">
        </div>
        <div class="col-sm-9 col-xs-12 paper-info">
          <h4>Vision-Based Gait Analysis for Senior Care</h4>
          <p>Evan Darke, Anin Sayana, Kelly Shen, David Xue, Jun-Ting Hsieh, <strong>Zelun Luo</strong>, Li-Jia Li, N, Lance Downing, Arnold Milstein, Li Fei-Fei</p>
          <p>ML4H: Machine Learning for Health, NeurIPS 2018, Montreal, Canada, December 8, 2018</p>
          <div>
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="bottom" title="" data-content="As the senior population rapidly increases, it is challenging yet crucial to provide effective long-term care for seniors who live at home or in senior care facilities. Smart senior homes, which have gained widespread interest in the healthcare community, have been proposed to improve the well-being of seniors living independently. In particular, non-intrusive, cost-effective sensors placed in these senior homes enable gait characterization, which can provide clinically relevant information including mobility level and early neurodegenerative disease risk. In this paper, we present a method to perform gait analysis from a single camera placed within the home. We show that we can accurately calculate various gait parameters, demonstrating the potential for our system to monitor the long-term gait of seniors and thus aid clinicians in understanding a patient’s medical profile." data-original-title="Abstract">Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/pdf/1812.00169.pdf">PDF</a>
          </div>
        </div>
      </div>

      <div class="row paper">
        <div class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig">
          <img src="publications/luo2018computer.gif" class="img-responsive" onerror="this.onerror=null;this.src='publications/luo2018computer.gif';">
        </div>
        <div class="col-sm-9 col-xs-12 paper-info">
          <h4>Computer Vision-based Descriptive Analytics of Seniors' Daily Activities for Long-term Health Monitoring</h4>
          <p><strong>Zelun Luo*</strong>, Jun-Ting Hsieh*, Niranjan Balachandar, Serena Yeung, Guido Pusiol, Jay Luxenberg, Grace Li, Li-Jia Li, N. Lance Downing, Arnold Milstein, Li Fei-Fei</p>
          <p>Machine Learning for Healthcare (MLHC) 2018, Stanford, CA, August 17-18, 2018</p>
          <div>
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="bottom" title="" data-content="One in twenty-five patients admitted to a hospital will suffer from a hospital acquired infection. If we can intelligently track healthcare staff, patients, and visitors, we can better understand the sources of such infections. We envision a smart hospital capable of increasing operational efficiency and improving patient care with less spending. In this paper, we propose a non-intrusive vision-based system for tracking people’s activity in hospitals. We evaluate our method for the problem of measuring hand hygiene compliance. Empirically, our method outperforms existing solutions such as proximity-based techniques and covert in-person observational studies. We present intuitive, qualitative results that analyze human movement patterns and conduct spatial analytics which convey our method’s interpretability. This work is a first step towards a computer-vision based smart hospital and demonstrates promising results for reducing hospital acquired infections." data-original-title="Abstract">Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank" href="https://static1.squarespace.com/static/59d5ac1780bd5ef9c396eda6/t/5b7373254ae23704e284bdf4/1534292778467/18.pdf">PDF</a>
          </div>
        </div>
      </div>

      <div class="row paper">
        <div class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig">
          <img src="publications/haque2017towards.png" class="img-responsive">
        </div>
        <div class="col-sm-9 col-xs-12 paper-info">
          <h4>Towards Vision-Based Smart Hospitals: A System for Tracking and Monitoring Hand Hygiene Compliance</h4>
          <p>Albert Haque, Michelle Guo, Alexandre Alahi, Serena Yeung, <strong>Zelun Luo</strong>, Alisha Rege, Amit Singh, Jeffrey Jopling, N. Lance Downing, William Beninati, Terry Platchek, Arnold Milstein, Li Fei-Fei</p>
          <p>Machine Learning for Healthcare (MLHC) 2017, Boston, MA, August 18-19, 2017</p>
          <div>
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="bottom" title="" data-content="Nations around the world face rising demand for costly long-term care for seniors. Patterns
                                              in seniors' activities of daily living, such as sleeping, sitting, standing, walking, etc. can
                                              provide caregivers useful clues regarding seniors' health. As the senior population continues
                                              to grow worldwide, continuous manual monitoring of seniors' daily activities will
                                              become more and more challenging for caregivers. Thus to improve caregivers' ability
                                              to assist seniors, an automated system for monitoring and analyzing patterns in seniors
                                              activities of daily living would be useful. A possible approach to implementing such a
                                              system involves wearable sensors, but this approach is intrusive and requires adherence by
                                              patients. In this paper, using a dataset we collected from an assisted-living facility for
                                              seniors, we present a novel computer vision-based approach that leverages nonintrusive,
                                              privacy-compliant multi-modal sensors and state-of-the-art computer vision techniques for
                                              continuous activity detection to remotely detect and provide long-term descriptive analytics
                                              of senior activities. These analytics include both qualitative and quantitative descriptions
                                              of senior daily activity patterns that can be interpreted by caregivers. Our work is progress
                                              towards a smart senior home that uses computer vision to support caregivers in senior
                                              healthcare to help meet the challenges of an aging worldwide population." data-original-title="Abstract">Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/pdf/1708.00163.pdf">PDF</a>
          </div>
        </div>
      </div>

      <div class="row paper">
        <div class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig">
          <img src="publications/luo2017computer.png" class="img-responsive">
        </div>
        <div class="col-sm-9 col-xs-12 paper-info">
          <h4>Computer Vision-based Approach to Maintain Independent Living for Seniors</h4>
          <p><strong>Zelun Luo</strong>, Alisha Rege, Guido Pusiol, Arnold Milstein, Li Fei-Fei, N. Lance Downing</p>
          <p>American Medical Informatics Association (AMIA), Washington, DC, November 4-8, 2017</p>
          <div>
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="bottom" title="" data-content="Recent progress in developing cost-effective sensors and machine learning techniques has enabled new AI-assisted solutions for human behavior understanding. In this work, we investigate the use of thermal and depth sensors for the detection of daily activities, lifestyle patterns, emotions, and vital signs, as well as the development of intelligent mechanisms for accurate situational assessment and rapid response. We demonstrate an integrated solution for remote monitoring, assessment, and support of seniors living independently at home." data-original-title="Abstract">Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank" href="publications/AMIA-Poster.pdf">Poster</a>
            <a class="btn btn-default btn-xs" target="_blank" href="publications/luo2017computer.pdf">Manuscript</a>
          </div>
        </div>
      </div>

      <div class="row paper paper-last">
        <div class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig">
          <img src="publications/yeung2015vision.png" class="img-responsive">
        </div>
        <div class="col-sm-9 col-xs-12 paper-info">
          <h4>Vision-Based Hand Hygiene Monitoring in Hospitals</h4>
          <p>Serena Yeung, Alexandre Alahi, <strong>Zelun Luo</strong>, Boya Peng, Albert Haque, Amit Singh, Terry Platchek,
            Arnold Milstein, Li Fei-Fei</p>
          <p>American Medical Informatics Association (AMIA), Chicago, November 12-16, 2016</p>
          <p>NIPS Workshop on Machine Learning for Healthcare, 2015</p>
          <div>
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="bottom" title="" data-content="Recent progress in developing cost-effective depth sensors has enabled new AI-assisted
                                              solutions such as assisted driving vehicles and smart spaces. Machine
                                              learning techniques have been successfully applied on these depth signals to perceive
                                              meaningful information about human behavior. In this work, we propose
                                              to deploy depth sensors in hospital settings and use computer vision methods to
                                              enable AI-assisted care. We aim to reduce visually-identifiable human errors such
                                              as hand hygiene compliance, one of the leading causes of Health Care-Associated
                                              Infection (HCAI) in hospitals." data-original-title="Abstract">Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank" href="http://ai.stanford.edu/~syyeung/resources/vision_hand_hh_nipsmlhc.pdf">PDF</a>
          </div>
        </div>
      </div>
    </div>

    <div class="row">
      <h3 class="subfield">Biomedical Imaging and Diagnosis</h3>
      <div class="row paper">
        <div class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig">
          <img src="publications/kandel2017label.png" class="img-responsive">
        </div>
        <div class="col-sm-9 col-xs-12 paper-info">
          <h4>Label-Free Tissue Scanner for Colorectal Cancer Screening</h4>
          <p>Mikhail E. Kandel, Shamira Sridharan, Jon Liang, <strong>Zelun Luo</strong>, Kevin Han, Virgilia Macias, Anish Shah,
            Roshan Patel, Krishnarao Tangella, Andre Kajdacsy-Balla, Grace Guzman, Gabriel Popescu</p>
          <p>Journal of Biomedical Optics, Opt. 22(6), 2017</p>
          <div>
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="bottom" title="" data-content="The current practice of surgical pathology relies on external contrast
                                              agents to reveal tissue architecture, which is then qualitatively examined
                                              by a trained pathologist. The diagnosis is based on the comparison with
                                              standardized empirical, qualitative assessments of limited objectivity.
                                              We propose an approach to pathology based on interferometric imaging of
                                              “unstained” biopsies, which provides unique capabilities for quantitative
                                              diagnosis and automation. We developed a label-free tissue scanner based
                                              on “quantitative phase imaging,” which maps out optical path length at
                                              each point in the field of view and, thus, yields images that are sensitive
                                              to the “nanoscale” tissue architecture. Unlike analysis of stained tissue,
                                              which is qualitative in nature and affected by color balance, staining
                                              strength and imaging conditions, optical path length measurements are
                                              intrinsically quantitative, i.e., images can be compared across different
                                              instruments and clinical sites. These critical features allow us to automate
                                              the diagnosis process. We paired our interferometric optical system with highly
                                              parallelized, dedicated software algorithms for data acquisition, allowing us
                                              to image at a throughput comparable to that of commercial tissue scanners while
                                              maintaining the nanoscale sensitivity to morphology. Based on the measured phase
                                              information, we implemented software tools for autofocusing during imaging, as
                                              well as image archiving and data access. To illustrate the potential of our
                                              technology for large volume pathology screening, we established an “intrinsic
                                              marker” for colorectal disease that detects tissue with dysplasia or colorectal
                                              cancer and flags specific areas for further examination, potentially improving
                                              the efficiency of existing pathology workflows. " data-original-title="Abstract">Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank" href="https://www.spiedigitallibrary.org/journals/Journal_of_Biomedical_Optics/volume-22/issue-6/066016/Label-free-tissue-scanner-for-colorectal-cancer-screening/10.1117/1.JBO.22.6.066016.full">PDF &amp; Website</a>
          </div>
        </div>
      </div>

      <div class="row paper">
        <div class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig">
          <img src="publications/majeed2016towards.png" class="img-responsive">
        </div>
        <div class="col-sm-9 col-xs-12 paper-info">
          <h4>Towards Quantitative Automated Histopathology of Breast Cancer using Spatial Light Interference Microscopy (SLIM)</h4>
          <p>Hassaan Majeed, Tan H Nguyen, Mikhail E Kandel, Kevin Han, <strong>Zelun Luo</strong>, Virgilia Macias,
            Krishnarao Tangella, Andre Balla, Minh N Do, Gabriel Popescu</p>
          <p>United States and Canadian Academy of Pathology (USCAP), Seattle, WA, March 12-18, 2016</p>
        </div>
      </div>

      <div class="row paper">
        <div class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig">
          <img src="publications/majeed2015breast.png" class="img-responsive">
        </div>
        <div class="col-sm-9 col-xs-12 paper-info">
          <h4>Breast Cancer Diagnosis using Spatial Light Interference Microscopy</h4>
          <p>Hassaan Majeed, Mikhail E Kandel, Kevin Han, <strong>Zelun Luo</strong>, Virgilia Macias, Krishnarao Tangella,
            Andre Balla, Gabriel Popescu</p>
          <p>Journal of Biomedical Optics, Opt. 20(11), 2015</p>
          <div>
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="bottom" title="" data-content="The standard practice in histopathology of breast cancers is to examine a hematoxylin and eosin (H&amp;E) stained tissue biopsy under a microscope to diagnose whether a lesion is benign or malignant. This determination is made based on a manual, qualitative inspection, making it subject to investigator bias and resulting in low throughput. Hence, a quantitative, label-free, and high-throughput diagnosis method is highly desirable. We present here preliminary results showing the potential of quantitative phase imaging for breast cancer screening and help with differential diagnosis. We generated phase maps of unstained breast tissue biopsies using spatial light interference microscopy (SLIM). As a first step toward quantitative diagnosis based on SLIM, we carried out a qualitative evaluation of our label-free images. These images were shown to two pathologists who classified each case as either benign or malignant. This diagnosis was then compared against the diagnosis of the two pathologists on corresponding H&amp;E stained tissue images and the number of agreements were counted. The agreement between SLIM and H&amp;E based diagnosis was 88% for the first pathologist and 87% for the second. Our results demonstrate the potential and promise of SLIM for quantitative, label-free, and high-throughput diagnosis." data-original-title="Abstract">Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank" href="http://light.ece.illinois.edu/wp-content/uploads/2015/10/Hassaan_JBO_20_11_111210.pdf">PDF</a>
            <a class="btn btn-default btn-xs" target="_blank" href="http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=2430724">Website</a>
          </div>
        </div>
      </div>

      <div class="row paper">
        <div class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig">
          <img src="publications/majeed2015high.png" class="img-responsive">
        </div>
        <div class="col-sm-9 col-xs-12 paper-info">
          <h4>High Throughput Imaging of Blood Smears using White Light Diffraction Phase Microscopy</h4>
          <p>Hassaan Majeed, Mikhail E Kandel, Basanta Bhaduri, Kevin Han, <strong>Zelun Luo</strong>, Krishnarao Tangella,
            Gabriel Popescu</p>
          <p>SPIE Photonics West: BiOS, San Francisco, CA, February 7-12, 2015</p>
          <div>
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="bottom" title="" data-content="While automated blood cell counters have made great progress in detecting abnormalities in blood, the lack of specificity for a particular disease, limited information on single cell morphology and intrinsic uncertainly due to high throughput in these instruments often necessitates detailed inspection in the form of a peripheral blood smear. Such tests are relatively time consuming and frequently rely on medical professionals tally counting specific cell types. These assays rely on the contrast generated by chemical stains, with the signal intensity strongly related to staining and preparation techniques, frustrating machine learning algorithms that require consistent quantities to denote the features in question. Instead we opt to use quantitative phase imaging, understanding that the resulting image is entirely due to the structure (intrinsic contrast) rather than the complex interplay of stain and sample. We present here our first steps to automate peripheral blood smear scanning, in particular a method to generate the quantitative phase image of an entire blood smear at high throughput using white light diffraction phase microscopy (wDPM), a single shot and common path interferometric imaging technique." data-original-title="Abstract">Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank" href="http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=2204110">
              PDF &amp; Website</a>
          </div>
        </div>
      </div>

      <div class="row paper">
        <div class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig">
          <img src="publications/majeed2015diagnosis.png" class="img-responsive">
        </div>
        <div class="col-sm-9 col-xs-12 paper-info">
          <h4>Diagnosis of Breast Cancer Biopsies using Quantitative Phase Imaging</h4>
          <p>Hassaan Majeed, Mikhail E Kandel, Kevin Han, <strong>Zelun Luo</strong>, Virgilia Macias, Krishnarao Tangella,
            Andre Balla, Gabriel Popescu</p>
          <p>SPIE Photonics West: BiOS, San Francisco, CA, February 7-12, 2015</p>
          <div>
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="bottom" title="" data-content="The standard practice in the histopathology of breast cancers is to examine a hematoxylin and eosin (H&amp;E) stained tissue biopsy under a microscope. The pathologist looks at certain morphological features, visible under the stain, to diagnose whether a tumor is benign or malignant. This determination is made based on qualitative inspection making it subject to investigator bias. Furthermore, since this method requires a microscopic examination by the pathologist it suffers from low throughput. A quantitative, label-free and high throughput method for detection of these morphological features from images of tissue biopsies is, hence, highly desirable as it would assist the pathologist in making a quicker and more accurate diagnosis of cancers. We present here preliminary results showing the potential of using quantitative phase imaging for breast cancer screening and help with differential diagnosis. We generated optical path length maps of unstained breast tissue biopsies using Spatial Light Interference Microscopy (SLIM). As a first step towards diagnosis based on quantitative phase imaging, we carried out a qualitative evaluation of the imaging resolution and contrast of our label-free phase images. These images were shown to two pathologists who marked the tumors present in tissue as either benign or malignant. This diagnosis was then compared against the diagnosis of the two pathologists on H&amp;E stained tissue images and the number of agreements were counted. In our experiment, the agreement between SLIM and H&amp;E based diagnosis was measured to be 88%. Our preliminary results demonstrate the potential and promise of SLIM for a push in the future towards quantitative, label-free and high throughput diagnosis." data-original-title="Abstract">Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank" href="http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=2204106">
              PDF &amp; Website</a>
          </div>
        </div>
      </div>

      <div class="row paper paper-last">
        <div class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig">
          <img src="publications/kandel2015cpp.png" class="img-responsive">
        </div>
        <div class="col-sm-9 col-xs-12 paper-info">
          <h4>C++ Software Integration for a High-throughput Phase Imaging Platform</h4>
          <p>Mikhail E Kandel, <strong>Zelun Luo</strong>, Kevin Han, Gabriel Popescu</p>
          <p>SPIE Photonics West: BiOS, San Francisco, CA, February 7-12, 2015</p>
          <div>
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="bottom" data-html="True" title="" data-content="The multi-shot approach in SLIM requires reliable, synchronous, and parallel operation of three independent hardware devices – not meeting these challenges results in degraded phase and slow acquisition speeds, narrowing applications to holistic statements about complex phenomena. The relative youth of quantitative imaging and the lack of ready-made commercial hardware and tools further compounds the problem as Higher level programming languages result in inflexible, experiment specific instruments limited by ill-fitting computational modules, resulting in a palpable chasm between promised and realized hardware performance. Furthermore, general unfamiliarity with intricacies such as background calibration, objective lens attenuation, along with spatial light modular alignment, makes successful measurements difficult for the inattentive or uninitiated. This poses an immediate challenge for moving our techniques beyond the lab to biologically oriented collaborators and clinical practitioners.
                <br />To meet these challenges, we present our new Quantitative Phase Imaging pipeline, with improved instrument performance, friendly user interface and robust data processing features, enabling us to acquire and catalog clinical datasets hundreds of gigapixels in size." data-original-title="Abstract">Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank" href="http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=2204094">
              PDF &amp; Website</a>
          </div>
        </div>
      </div>
    </div>


  </div>
</section>

<!--Google Analytics-->
<script async="" src="https://www.google-analytics.com/analytics.js"></script><script>
  (function (i, s, o, g, r, a, m) {
    i['GoogleAnalyticsObject'] = r;
    i[r] = i[r] || function () {
          (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date();
    a = s.createElement(o),
        m = s.getElementsByTagName(o)[0];
    a.async = 1;
    a.src = g;
    m.parentNode.insertBefore(a, m)
  })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

  ga('create', 'UA-100690611-1', 'auto');
  ga('send', 'pageview');
</script>

<!-- jQuery -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js"></script>

<!-- Bootstrap Core JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

<!-- Custom JavaScript -->
<script src="js/effects.js"></script>




<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML&amp;delayStartupUntil=configured"></script><script id="texAllTheThingsPageScript" type="text/javascript" src="chrome-extension://cbimabofgmfdkicghcadidpemeenbffn/js/pageScript.js" inlinemath="[[&quot;$&quot;,&quot;$&quot;],[&quot;[;&quot;,&quot;;]&quot;]]" displaymath="[[&quot;$$&quot;,&quot;$$&quot;],[&quot;\\[&quot;,&quot;\\]&quot;]]" skiptags="[&quot;script&quot;,&quot;noscript&quot;,&quot;style&quot;,&quot;textarea&quot;,&quot;pre&quot;,&quot;code&quot;]"></script></body><style id="stylus-4" type="text/css" class="stylus">#docs-chrome{background: #333 none repeat scroll 0% 0% !important; color: #DDD !important;}
  
  #gb#gb a.gb_b, /* Account email in top right corner */
  .docs-presence-plus-widget-status /* x total viewers text */
  {color: #DDD !important;}
  
  /* Document title box */
  .docs-title-input{
    background-color: #333333 !important;

    font-style: normal !important;
  }
  .docs-title-input:focus{
    color: white !important;

  }

goog-inline-block jfk-button jfk-button-standard docs-titlebar-button{
background-color: white}

.docs-material-appbar #docs-header .docs-titlebar-buttons{
background-color: #333333
}
  .docs-title-untitled{display: none !important;}
  
  /* Fix for menu bar text when hover */
  .goog-control-hover{color: black !important;}
  
  /* Main toolbar */
  #docs-toolbar-wrapper{
    border-top: 1px solid #151516 !important;
    border-bottom: 1px solid #040404 !important;
    background-image: linear-gradient(#333333, #313131) !important;
    box-shadow: inset 0 1px rgba(255,255,255,0.15) !important;
  }
  /* i really want this to be global */
  /* Toolbar's vertical dividers */
  .goog-toolbar-separator.goog-inline-block{border-left: 1px solid #222 !important;}
  
.navigation-widget-hat-title {
color: white}

  /* A very simple/lazy fix to get toolbar item colors to look good on a dark background */
  .goog-toolbar-button,
  .goog-toolbar-toggle-button,
  .goog-toolbar-menu-button:not([id="textColorButton"]),
  .goog-toolbar-combo-button,
  .docs-icon-folder-solid{
    -webkit-filter: invert(1) !important;
    filter: invert(1) !important;
  }
  
  /* Fixes for suggestion mode when editing own doc, viewing mode when editing others' docs */
  #docs-toolbar-mode-switcher, #docs-access-level-indicator{background-color: transparent !important; color: black !important;}
  .docs-icon-mode-review-white, .docs-icon-mode-view-white{filter: invert(1) !important;}
  
  /* Background behind the document */
  #docs-editor{background: #404143 none repeat scroll 0% 0% !important;}

/* BEGIN Sepia Background */

  .kix-page,
  .kix-page-compact,
  .kix-page-content-wrapper,
  .kix-paginateddocumentplugin,
  .kix-paginateddocumentplugin-compact-mode
  .docs-ui-unprintable {
    background-color: #F3EAD9 !important;
  }

  .kix-paginateddocumentplugin-background {
    display: none;
  }

/* END Sepia Background */
  
  /* Document drop-shadow removal */
/* .kix-page-paginated{box-shadow: none !important;} */</style></html>
